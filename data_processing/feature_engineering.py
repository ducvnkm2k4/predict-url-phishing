import pandas as pd
import numpy as np
import re
from urllib.parse import urlparse
import tldextract
from multiprocessing import Pool, cpu_count
from tqdm import tqdm  # Hiá»ƒn thá»‹ tiáº¿n trÃ¬nh
from collections import Counter
import math
from pre_processing.delete_duplicate import delete_duplicate
# Load dá»¯ liá»‡u
data_train = pd.read_csv('dataset/raw/data_train_raw.csv')
data_test = pd.read_csv('dataset/raw/data_test_raw.csv')

# Äá»c danh sÃ¡ch top 100k domain tá»« Tranco
top_100k_tranco_list = set(pd.read_csv('dataset/tranco_list/tranco_5897N.csv', header=None).iloc[:, 1].tolist())

# Äá»c báº£ng xÃ¡c suáº¥t kÃ½ tá»± (ngram)
char_probabilities = pd.read_csv('dataset/tranco_list/char_probabilities.csv').set_index('Character')['Probability'].to_dict()

# CÃ¡c danh sÃ¡ch keyword vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t
special_chars = "`%^&*;@!?#=+$"
special_chars_domain=".-_"
hex_pattern = re.compile(r'[a-fA-F0-9]{10,}')
common_keywords = {"password", "login", "secure", "account", "index", "token", "signin", "update", "verify", "auth", "security"}
sensitive_keywords = {"confirm", "submit", "payment", "invoice", "billing", "transaction", "transfer", "refund", "wire"}
short_url_services = {"bit.ly", "goo.gl", "tinyurl.com", "is.gd", "t.co", "ow.ly", "cutt.ly", "shrtco.de", "rebrand.ly", "lnkd.in"}
redirect_keywords = {"redirect=", "url=", "next=", "dest=", "destination=", "forward=", "go=", "to="}
ipv6_pattern = re.compile(r'^([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}$')
ip_pattern = re.compile(r'^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$')

# **HÃ m trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng**
def extract_features(params):
    url, label = params  # Nháº­n tuple (url, label)
    url=url.strip("'\"")
    parsed_url = urlparse(url)

    url_for_parsing = url if parsed_url.scheme in {"http","https"} else "http://"+url
    parsed_url=urlparse(url_for_parsing)

    domain = parsed_url.hostname
    extracted = tldextract.extract(url)
    domain_extracted = extracted.registered_domain
    length = len(url)

    char_counts = Counter(domain)

    total_chars = sum(char_counts.values())
    domain_char_probabilities = {char: count / total_chars for char, count in char_counts.items()}

    # TÃ­nh toÃ¡n cÃ¡c Ä‘áº·c trÆ°ng trÃªn url
    f1 = length  # length: Ä‘á»™ dÃ i URL
    f2 = sum(1 for char in url if char in special_chars) / length  # tachar: tá»· lá»‡ kÃ½ tá»± Ä‘áº·c biá»‡t trong URL
    f3 = int(any(kw in url.lower() for kw in common_keywords))  # hasKeyWords: URL chá»©a tá»« khÃ³a phá»• biáº¿n
    f4 = int(any(kw in url.lower() for kw in sensitive_keywords))  # hasspecKW: URL chá»©a tá»« khÃ³a nháº¡y cáº£m
    f5 = sum(len(match) for match in re.findall(hex_pattern, url)) / length  # tahex: tá»· lá»‡ chuá»—i hex trong URL
    f6 = sum(1 for char in url if char.isdigit()) / length  # tadigit: tá»· lá»‡ chá»¯ sá»‘ trong URL
    f7 = url.count('.')  # numDots: sá»‘ dáº¥u cháº¥m trong URL
    f8 = url.count('/') / length  # taslash: tá»· lá»‡ dáº¥u '/' trong URL
    f9 = sum(1 for char in url if char.isupper())  # countUpcase: sá»‘ kÃ½ tá»± in hoa trong URL
    f10 = sum(1 for char in url if char.lower() in "aeiou") / length  # numvo: tá»· lá»‡ nguyÃªn Ã¢m trong URL
    f11 = sum(1 for char in url if char.isalpha() and char.lower() not in "aeiou") / length  # numco: tá»· lá»‡ phá»¥ Ã¢m trong URL
    f12 = int('\\' in url)  # backslash: URL chá»©a kÃ½ tá»± '\'
    f13 = int(any(len(sub) > 30 for sub in re.findall(r'\S+', url)))  # maxsub30: URL chá»©a chuá»—i con dÃ i >30 kÃ½ tá»±

    f14 = len(parsed_url.path) / length if parsed_url.path else 0  # rapath: Ä‘á»™ dÃ i Ä‘Æ°á»ng dáº«n so vá»›i toÃ n bá»™ URL
    f15 = 1 if urlparse(url).scheme in {"http", "https"} or parsed_url.netloc.startswith("www.") else 0 # haspro 
    f16 = int(parsed_url.path.lower().endswith(".exe"))  # hasExe: URL káº¿t thÃºc báº±ng .exe
    f17 = int(any(kw in parsed_url.query.lower() for kw in redirect_keywords))  # redirect: URL chá»©a tá»« khÃ³a Ä‘iá»u hÆ°á»›ng
    f18 = int(any(kw in parsed_url.query.lower() for kw in ["ref=", "cdm=", "track=", "utm="]) and "href=" not in parsed_url.query.lower() and "notrack=1" not in parsed_url.query.lower())# hasref: URL chá»©a tham sá»‘ theo dÃµi

    is_domain_ip = re.fullmatch(ip_pattern, domain)
    # Äáº·c trÆ°ng tÃªn miá»n
    f19 = int(is_domain_ip is not None)  # hasIP: URL chá»©a Ä‘á»‹a chá»‰ IP
    f20 = int(parsed_url.port is not None)  # hasport: URL cÃ³ chá»©a sá»‘ cá»•ng
    f21 = 0 if is_domain_ip else domain.count('.') - 1 # numsdm: sá»‘ lÆ°á»£ng subdomain trong tÃªn miá»n
    f22 = len(domain) / length if domain else 0  # radomain: tá»· lá»‡ Ä‘á»™ dÃ i cá»§a domain so vá»›i tÃªn miá»n
    f23= int(domain in short_url_services)  # tinyUrl: URL lÃ  dá»‹ch vá»¥ rÃºt gá»n
    f24 = sum(1 for char in domain if char in "aeiou") / len(domain) if domain else 0  # tanv: tá»· lá»‡ nguyÃªn Ã¢m trong tÃªn miá»n
    f25 = sum(1 for char in domain if char.isalpha() and char.lower() not in "aeiou") / len(domain) if domain else 0  # tanco: tá»· lá»‡ phá»¥ Ã¢m trong tÃªn miá»n
    f26 = sum(1 for char in domain if char.isdigit()) / len(domain) if domain else 0  # tandi: tá»· lá»‡ chá»¯ sá»‘ trong tÃªn miá»n
    f27 = sum(1 for char in domain if char in special_chars_domain) / len(domain) if domain else 0  # tansc: tá»· lá»‡ kÃ½ tá»± Ä‘áº·c biá»‡t trong tÃªn miá»n
    f28 = sum(len(match) for match in re.findall(hex_pattern, domain)) / len(domain) if domain else 0  # tanhe: tá»· lá»‡ chuá»—i hex trong tÃªn miá»n
    f29 = int(domain[0].isdigit()) if domain else 0  # is_digit: tÃªn miá»n báº¯t Ä‘áº§u báº±ng sá»‘
    f30 = len(domain) if domain else 0  # len: Ä‘á»™ dÃ i tÃªn miá»n

    f31 = -sum((p * math.log(p))/math.log(len(domain)) for p in domain_char_probabilities.values()) if domain else 0  # ent_char: entropy cá»§a kÃ½ tá»± trong tÃªn miá»n
    f32 = sum(domain.count(c) * char_probabilities.get(c, 0) for c in domain) / len(domain) if domain and char_probabilities else 0
    
    f33 = 0 if is_domain_ip else int( domain_extracted in top_100k_tranco_list)  # rank: tÃªn miá»n thuá»™c top 100k cá»§a Tranco
    f34 = 0 if is_domain_ip else int(extracted.suffix in {"com", "net", "org", "edu", "gov"})  # tld: tÃªn miá»n thuá»™c TLD phá»• biáº¿n

    return [f1, f2, f3,f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, f21, f22, f23, f24,
            f25, f26, f27, f28, f29, f30, f31, f32, f33, f34, label]  # GÃ¡n label cuá»‘i cÃ¹ng

# **HÃ m cháº¡y multiprocessing**
def parallel_feature_extraction(url_list, label_list):
    num_cores = cpu_count()  # DÃ¹ng (N-1) CPU cores
    print(f"â³ Äang xá»­ lÃ½ vá»›i {num_cores} CPU cores...")

    params = list(zip(url_list, label_list))  # Táº¡o danh sÃ¡ch tuple (url, label)

    with Pool(num_cores) as pool:
        results = list(tqdm(pool.imap(extract_features, params), total=len(url_list), desc="Extracting Features"))

    return results

if __name__ == "__main__":
    # Cháº¡y feature extraction trÃªn táº­p train & test
    extracted_features_train = parallel_feature_extraction(data_train['url'], data_train['label'])
    extracted_features_test = parallel_feature_extraction(data_test['url'], data_test['label'])

    feature_names = ["length", "tachar", "hasKeyWords", "hasspecKW", "tahex", "tadigit", "numDots","taslash", "countUpcase", "numvo", "numco", "backslash", "maxsub30", "rapath","haspro", "hasExe", "redirect", "hasref", "hasIP", "hasport", "numsdm", "radomain","tinyUrl", "tanv", "tanco", "tandi", "tansc", "tanhe", "is_digit", "domain_len", "ent_char", "eod", "rank", "tld", "label"]


    data_train_feature = pd.DataFrame(extracted_features_train, columns=feature_names)
    data_test_feature = pd.DataFrame(extracted_features_test, columns=feature_names)

    delete_duplicate(data_train=data_train_feature,data_test=data_test_feature)
    # # LÆ°u káº¿t quáº£
    # data_train_feature.to_csv('dataset/feature/data_train.csv', index=False)
    # data_test_feature.to_csv('dataset/feature/data_test.csv', index=False)
    print("âœ… TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng hoÃ n thÃ nh! ğŸš€")
